{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f010cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pastalover/miniconda3/envs/sisyphus_context/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from router import router_api\n",
    "# router is a file with api keys configured\n",
    "router_api('context')\n",
    "import warnings\n",
    "import logging\n",
    "from typing import Callable, Literal\n",
    "from functools import partial\n",
    "\n",
    "import dspy\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from sisyphus.chain import Filter, Writer\n",
    "from sisyphus.utils.helper_functions import get_plain_articledb, get_create_resultdb, get_title_abs, render_docs\n",
    "from sisyphus.urgent.json_schemas import StrengthRecords, PhaseRecords, GrainSizeRecords\n",
    "import sisyphus.urgent.json_schemas_no_syn\n",
    "from sisyphus.chain import Paragraph, ParagraphExtend\n",
    "from sisyphus.strategy.utils import get_paras_with_props, get_synthesis_paras\n",
    "from sisyphus.urgent.properties_extraction import extract_func_wrapper\n",
    "from sisyphus.urgent.entity_resolution import entity_resolution_llms, entity_resolution_rule\n",
    "from sisyphus.urgent.merge import merge, REFERRED\n",
    "\n",
    "from prompt import simple_prompt_template, simple_prompt_template_no_syn, phase_instruction, strength_instruction, grain_size_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31417b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore', category=RuntimeWarning, module='pydantic') # the case that we convert json string to python object trigger pydantic warning\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.FileHandler('more_than_20.log')\n",
    "handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model='gpt-4.1')\n",
    "database_name = 'labeled'\n",
    "result_db_name = 'contextualized'\n",
    "merged_output_file = 'contextualized_merged.jsonl'\n",
    "target_dir = 'TEST_FILES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b968dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_extractor(prompt_template, output_model, property_labels, context_labels, instruction, type_name, has_synthesis=True):\n",
    "    \"\"\"Helper to create an extractor partial to avoid repeated boilerplate.\"\"\"\n",
    "    # Wrap the newer `extract_property_` which composes prompt + model\n",
    "    return partial(\n",
    "        extract_property_,\n",
    "        property_labels=property_labels,\n",
    "        context_labels=context_labels,\n",
    "        has_synthesis=has_synthesis,\n",
    "        prompt_template=prompt_template,\n",
    "        instruction=instruction,\n",
    "        chat_model=model,\n",
    "        output_model=output_model,\n",
    "        type=type_name,\n",
    "    )\n",
    "\n",
    "def extract_property_(\n",
    "        paragraphs: list[Paragraph],\n",
    "        property_labels: list[str],\n",
    "        context_labels: list[str],\n",
    "        has_synthesis: bool,\n",
    "        prompt_template: ChatPromptTemplate,\n",
    "        instruction: str,\n",
    "        chat_model: ChatOpenAI,\n",
    "        output_model: BaseModel,\n",
    "        **kwargs \n",
    ") -> ParagraphExtend:\n",
    "    chain = prompt_template | chat_model.with_structured_output(output_model, method='json_schema')\n",
    "\n",
    "    target_paras = get_paras_with_props(paragraphs, *property_labels, *context_labels)\n",
    "    is_existence = get_paras_with_props(paragraphs, *property_labels)\n",
    "    if not is_existence:\n",
    "        return []\n",
    "\n",
    "    if has_synthesis:\n",
    "        paragraph = ParagraphExtend.from_paragraphs(target_paras, **kwargs)\n",
    "        syn_para = ParagraphExtend.from_paragraphs(get_synthesis_paras(paragraphs)) \n",
    "        res = chain.invoke(\n",
    "            {\n",
    "                'property_instruction': instruction,\n",
    "                'synthesis_para': syn_para.page_content,\n",
    "                'property': paragraph.page_content\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        paragraph = ParagraphExtend.from_paragraphs(target_paras, **kwargs)\n",
    "        res = chain.invoke(\n",
    "            {\n",
    "                'property_instruction': instruction,\n",
    "                'property': paragraph.page_content\n",
    "            }\n",
    "        )\n",
    "\n",
    "    paragraph.set_data(res.records)\n",
    "    if paragraph.data:\n",
    "        return [paragraph]\n",
    "    return []\n",
    "\n",
    "# Extractors when synthesis paragraphs exist\n",
    "extract_phase = _make_extractor(\n",
    "    simple_prompt_template, PhaseRecords,\n",
    "    ['phase'], ['composition'], phase_instruction, 'phase_extraction'\n",
    ")\n",
    "extract_strength = _make_extractor(\n",
    "    simple_prompt_template, StrengthRecords,\n",
    "    ['strength'], ['composition', 'strain_rate'], strength_instruction, 'strength_extraction'\n",
    ")\n",
    "extract_grainsize = _make_extractor(\n",
    "    simple_prompt_template, GrainSizeRecords,\n",
    "    ['grain_size'], ['composition'], grain_size_instruction, 'grain_size_extraction'\n",
    ")\n",
    "\n",
    "# Extractors when synthesis paragraphs do NOT exist\n",
    "extract_phase_no_syn = _make_extractor(\n",
    "    simple_prompt_template_no_syn, sisyphus.urgent.json_schemas_no_syn.PhaseRecords,\n",
    "    ['phase'], ['composition'], phase_instruction, 'phase_extraction_no_syn', has_synthesis=False\n",
    ")\n",
    "extract_strength_no_syn = _make_extractor(\n",
    "    simple_prompt_template_no_syn, sisyphus.urgent.json_schemas_no_syn.StrengthRecords,\n",
    "    ['strength'], ['composition', 'strain_rate'], strength_instruction, 'strength_extraction_no_syn', has_synthesis=False\n",
    ")\n",
    "extract_grainsize_no_syn = _make_extractor(\n",
    "    simple_prompt_template_no_syn, sisyphus.urgent.json_schemas_no_syn.GrainSizeRecords,\n",
    "    ['grain_size'], ['composition'], grain_size_instruction, 'grain_size_extraction_no_syn', has_synthesis=False\n",
    ")\n",
    "\n",
    "\n",
    "def extract(paragraphs: list[Paragraph]):\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    merged = []\n",
    "    if syn_paras:=get_synthesis_paras(paragraphs):\n",
    "        extractors = (extract_phase, extract_strength, extract_grainsize)\n",
    "        expected_names = ['extract_phase', 'extract_strength', 'extract_grainsize']\n",
    "        name_to_result = {name: [] for name in expected_names}\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=3) as ex:\n",
    "            future_to_name = {ex.submit(fn, paragraphs): name for fn, name in zip(extractors, expected_names)}\n",
    "            for fut in as_completed(future_to_name):\n",
    "                name = future_to_name[fut]\n",
    "                try:\n",
    "                    name_to_result[name] = fut.result() or []\n",
    "                except Exception:\n",
    "                    # keep failure of one extractor from stopping others\n",
    "                    name_to_result[name] = []\n",
    "\n",
    "        paras = (name_to_result.get('extract_phase', []) +\n",
    "                 name_to_result.get('extract_strength', []) +\n",
    "                 name_to_result.get('extract_grainsize', []))\n",
    "\n",
    "        records_groups = []\n",
    "        records = []\n",
    "        for para in paras:\n",
    "            records_groups.append(para.data)\n",
    "            records_groups = [group for group in records_groups if group]  # filter out empty groups\n",
    "            records.extend(para.data)\n",
    "            records = [record for record in records if record]  # filter out empty records\n",
    "        if records:\n",
    "            metadata_referred = [record.metadata.model_dump() for record in records if getattr(record, REFERRED)]\n",
    "            metadata_all = [record.metadata.model_dump() for record in records]\n",
    "            metadata_groups = [[record.metadata.model_dump() for record in group if not getattr(record, REFERRED)] for group in records_groups]\n",
    "\n",
    "            if len(metadata_groups) > 1:\n",
    "                # safety cutoff ~ 20\n",
    "                if sum(len(group) for group in metadata_groups) > 20:\n",
    "                    logger.info('Skipping for DOI: %s', paragraphs[0].metadata.get('doi'))\n",
    "                    return paras\n",
    "                syn_text = ParagraphExtend.from_paragraphs(syn_paras).page_content\n",
    "                resolved_metadata_groups = entity_resolution_llms(metadata_groups, model, syn_text) + entity_resolution_rule(metadata_referred, ['composition', 'label'])\n",
    "            else:  # fallback to rule-based if only one group\n",
    "                resolved_metadata_groups = entity_resolution_rule(metadata_all, ['composition', 'label'])\n",
    "\n",
    "            merged = merge(resolved_metadata_groups, records)\n",
    "\n",
    "    else:\n",
    "        extractors = (extract_phase_no_syn, extract_strength_no_syn, extract_grainsize_no_syn)\n",
    "        expected_names = ['extract_phase_no_syn', 'extract_strength_no_syn', 'extract_grainsize_no_syn']\n",
    "        name_to_result = {name: [] for name in expected_names}\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=3) as ex:\n",
    "            future_to_name = {ex.submit(fn, paragraphs): name for fn, name in zip(extractors, expected_names)}\n",
    "            for fut in as_completed(future_to_name):\n",
    "                name = future_to_name[fut]\n",
    "                try:\n",
    "                    name_to_result[name] = fut.result() or []\n",
    "                except Exception:\n",
    "                    name_to_result[name] = []\n",
    "\n",
    "        paras = (name_to_result.get('extract_phase_no_syn', []) +\n",
    "                 name_to_result.get('extract_strength_no_syn', []) +\n",
    "                 name_to_result.get('extract_grainsize_no_syn', []))\n",
    "\n",
    "        records = []\n",
    "        for para in paras:\n",
    "            records.extend(para.data)\n",
    "            records = [record for record in records if record]  # filter out empty records\n",
    "        if records:\n",
    "            metadata_all = [record.metadata.model_dump() for record in records]\n",
    "            resolved_metadata_groups = entity_resolution_rule(metadata_all, ['composition', 'label'])\n",
    "\n",
    "            merged = merge(resolved_metadata_groups, records)\n",
    "\n",
    "    if merged:\n",
    "        with open(merged_output_file, 'a') as f:\n",
    "            import json\n",
    "            to_write = {\n",
    "                'DOI': paragraphs[0].metadata.get('doi'),\n",
    "                'records': merged\n",
    "            }\n",
    "            json_str = json.dumps(to_write, ensure_ascii=False)\n",
    "            f.write(json_str + \"\\n\")\n",
    "    return paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22667d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_database = get_plain_articledb(database_name)\n",
    "labeled_getter = Filter(labeled_database)\n",
    "result_db = get_create_resultdb(result_db_name)\n",
    "writer = Writer(result_db)\n",
    "def load(docs):\n",
    "    return [Paragraph.from_labeled_document(doc, id_) for id_, doc in enumerate(docs)]\n",
    "extract_chain = labeled_getter + load + extract + writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd3928a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:11<00:00,  3.97s/it]\n"
     ]
    }
   ],
   "source": [
    "from sisyphus.chain.chain_elements import run_chains_with_extarction_history_multi_threads\n",
    "run_chains_with_extarction_history_multi_threads(extract_chain, target_dir, 5, result_db_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sisyphus_context",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
