{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from router import router_api\n",
    "# router is a file with api keys configured\n",
    "router_api('paragraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1784dc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pastalover/miniconda3/envs/sisyphus_context/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import logging\n",
    "from typing import Callable, Literal\n",
    "from functools import partial\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import dspy\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from sisyphus.chain.chain_elements import Filter, Writer, run_chains_with_extarction_history_multi_threads\n",
    "from sisyphus.utils.helper_functions import get_plain_articledb, get_create_resultdb, get_title_abs, render_docs, render_docs_without_title\n",
    "from sisyphus.urgent.json_schemas_no_syn import StrengthRecords, PhaseRecords, GrainSizeRecords, SynthesisRecords\n",
    "from sisyphus.chain import Paragraph, ParagraphExtend\n",
    "from sisyphus.strategy.utils import get_paras_with_props, get_synthesis_paras\n",
    "from sisyphus.urgent.properties_extraction import extract_func_wrapper\n",
    "from sisyphus.urgent.entity_resolution import entity_resolution_llms, entity_resolution_rule\n",
    "from sisyphus.urgent.merge import merge, REFERRED\n",
    "from sisyphus.heas.prompt import (\n",
    "    INSTRUCTION_TEMPLATE,\n",
    "    phase_instruction,\n",
    "    strength_instruction,\n",
    "    grain_size_instruction\n",
    ")\n",
    "from sisyphus.heas.synthesis import get_synthesis_prompt\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, module='pydantic') # the case that we convert json string to python object trigger pydantic warning\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pydantic') \n",
    "\n",
    "model = ChatOpenAI(temperature=0, model='gpt-4.1', max_tokens=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0c78bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = 'labeled'\n",
    "result_db_name = 'heas_paragraph_wise'\n",
    "target_dir = 'TEST_FILES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7fb3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_prompt_template_no_syn = ChatPromptTemplate.from_messages([\n",
    "    ('user',\"\"\"\n",
    "You are required to extract material information from text provided below and ouput desired format which generally a list of dictionaries, and for each includes metadata and property/synthesis information. Return empty list if no property/synthesis found. Specifically, the metadata has structure as follows:\n",
    "metadata: {{\n",
    "    \"composition\": \"%s\",\n",
    "    \"label\": \"%s\",\n",
    "    }}\n",
    "e.g., metadata: {{\n",
    "    \"composition\": \"Mn0.2CoCrNi@at\",\n",
    "    \"label\": \"A1\",\n",
    "    }}\n",
    "Specifically for composition:\n",
    "### **Composition Format:**.  \n",
    "    - Use `@at` to denote atomic percent (at%) and `@wt` for weight percent (wt%). For simple alloys, keep original nominal composition + basis marker.\n",
    "        - Example: `AlCoCrFeNi2.5@at`, `AlCoCrFeNi2.1@wt`\n",
    "    - For composites, e.g., 1 wt% AlN nanoparticles added to AlCoCrFeNi (at. %)\n",
    "        - composition: `AlCoCrFeNi@at+AlN@wt[1%]`\n",
    "### For label:\n",
    "    - Use the same as author designated in text (e.g., MA-1; Ag; An-300), do not add extra descriptive description.\n",
    "### For synthesis:\n",
    "1. Multiple Routes Handling\n",
    "- When authors describe varied processing parameters (e.g., different temperatures, times, compositions), extract **separate routes** for each distinct sample produced\n",
    "- Each route should correspond to a unique sample that resulted from the processing\n",
    "- Routes that produce different samples and routes that describe alternative methods for the same sample should all be extracted separately.\n",
    "\n",
    "For property/synthesis specific instruction: \n",
    "{property_instruction}\n",
    "\n",
    "Property/Synthesis section:\n",
    "{property}\n",
    "\"\"\")\n",
    "]\n",
    ")\n",
    "\n",
    "def extract_property_(\n",
    "        paragraphs: list[Paragraph],\n",
    "        property_labels: list[str],\n",
    "        context_labels: list[str],\n",
    "        instruction: str,\n",
    "        chat_model: ChatOpenAI,\n",
    "        output_model: BaseModel,\n",
    "        **kwargs \n",
    ") -> ParagraphExtend:\n",
    "    chain = simple_prompt_template_no_syn | chat_model.with_structured_output(output_model, method='json_schema')\n",
    "\n",
    "    target_paras = get_paras_with_props(paragraphs, *property_labels, *context_labels)\n",
    "    is_existence = get_paras_with_props(paragraphs, *property_labels)\n",
    "    if not is_existence:\n",
    "        return\n",
    "    paragraph = ParagraphExtend.from_paragraphs(target_paras, **kwargs)\n",
    "    res = chain.invoke(\n",
    "        {\n",
    "            'property_instruction': instruction,\n",
    "            'property': paragraph.page_content\n",
    "        }\n",
    "    )\n",
    "\n",
    "    paragraph.set_data(res.records)\n",
    "    return paragraph\n",
    "\n",
    "def extract_synthesis_(\n",
    "        paragraphs: list[Paragraph],\n",
    "        context_labels: list[str],\n",
    "        lm: dspy.LM,\n",
    "        chat_model: ChatOpenAI,\n",
    "        output_model: BaseModel,\n",
    "        **kwargs\n",
    "):\n",
    "    syn_paras = get_synthesis_paras(paragraphs)\n",
    "    context_paras = get_paras_with_props(paragraphs, *context_labels)\n",
    "    if not syn_paras:\n",
    "        return\n",
    "    synthesis_prompt = get_synthesis_prompt(render_docs_without_title(syn_paras), lm=lm)\n",
    "    chain = simple_prompt_template_no_syn | chat_model.with_structured_output(output_model, method='json_schema')\n",
    "    paragraph = ParagraphExtend.from_paragraphs(syn_paras + context_paras, **kwargs)\n",
    "    res = chain.invoke(\n",
    "        {\n",
    "            'property_instruction': synthesis_prompt,\n",
    "            'property': paragraph.page_content\n",
    "        }\n",
    "    )\n",
    "    paragraph.set_data(res.records)\n",
    "    return paragraph\n",
    "\n",
    "\n",
    "def extract(paragraphs: list[Paragraph]):\n",
    "    extract_strength = partial(\n",
    "        extract_property_,\n",
    "        property_labels=['strength'],\n",
    "        context_labels=['composition', 'strain_rate'],\n",
    "        instruction=strength_instruction,\n",
    "        chat_model=model,\n",
    "        output_model=StrengthRecords\n",
    "    )\n",
    "    extract_phase = partial(\n",
    "        extract_property_,\n",
    "        property_labels=['phase'],\n",
    "        context_labels=['composition'],\n",
    "        instruction=phase_instruction,\n",
    "        chat_model=model,\n",
    "        output_model=PhaseRecords\n",
    "    )\n",
    "    extract_grain_size = partial(\n",
    "        extract_property_,\n",
    "        property_labels=['grain_size'],\n",
    "        context_labels=['composition'],\n",
    "        instruction=grain_size_instruction,\n",
    "        chat_model=model,\n",
    "        output_model=GrainSizeRecords\n",
    "    )\n",
    "    extract_synthesis = partial(\n",
    "        extract_synthesis_,\n",
    "        context_labels=['composition', 'processing_parameters'],\n",
    "        lm=dspy.LM('openai/gpt-4.1'),\n",
    "        chat_model=model,\n",
    "        output_model=SynthesisRecords\n",
    "    )\n",
    "    extractors = [\n",
    "        extract_strength,\n",
    "        extract_phase,\n",
    "        extract_grain_size,\n",
    "        extract_synthesis\n",
    "    ]\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        result_paras = list(executor.map(lambda func: func(paragraphs), extractors))\n",
    "        result_paras = [para for para in result_paras if para]\n",
    "    merged = []\n",
    "    records = []\n",
    "    for para in result_paras:\n",
    "        records.extend(para.data)\n",
    "        records = [record for record in records if record]  # filter out empty records\n",
    "    if records:\n",
    "        metadata_all = [record.metadata.model_dump() for record in records]\n",
    "        resolved_metadata_groups = entity_resolution_rule(metadata_all, ['composition', 'label'])\n",
    "        merged = merge(resolved_metadata_groups, records)\n",
    "\n",
    "    if merged:\n",
    "        import json\n",
    "        try:\n",
    "            with open('heas_paragraph_wised.json', 'r') as f:\n",
    "                data = json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            data = []\n",
    "    \n",
    "        # Prepare new records\n",
    "        to_write = [{\n",
    "            'doi': paragraphs[0].metadata.get('doi'),\n",
    "            'Record': record\n",
    "        } for record in merged]\n",
    "        \n",
    "        # Extend and write back\n",
    "        data.extend(to_write)\n",
    "        \n",
    "        with open('heas_paragraph_wised.json', 'w') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return result_paras\n",
    "\n",
    "    result_db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79251abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_plain_articledb(database_name)\n",
    "getter = Filter(db)\n",
    "result_db = get_create_resultdb(result_db_name)\n",
    "writer = Writer(result_db)\n",
    " \n",
    "def load(docs):\n",
    "    return [Paragraph.from_labeled_document(doc, id_) for id_, doc in enumerate(docs)]\n",
    "\n",
    "from functools import partial\n",
    "chain = getter + load + extract + writer\n",
    "debug_chain = getter + load + extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d5976",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# debug_chain.compose('10.1002&sol;adem.201900587.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60df334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:14<00:00,  7.42s/it]\n"
     ]
    }
   ],
   "source": [
    "run_chains_with_extarction_history_multi_threads(chain, target_dir, 5, 'complete synthesis route', given_names=['10.1002&sol;adem.201900587.html', '10.1016&sol;j.jmrt.2024.04.249.html'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sisyphus_context",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
