{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77bf420c",
   "metadata": {},
   "source": [
    "##### assumed you have create database from the test files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8878ff",
   "metadata": {},
   "source": [
    "## Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 309.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from sisyphus.utils.helper_functions import get_plain_articledb, get_create_resultdb\n",
    "from sisyphus.chain.label import BaseLabeler, Labeling, save_labeled_paras_wrapper\n",
    "from sisyphus.chain.chain_elements import Filter, Writer, run_chains_with_extarction_history_multi_threads\n",
    "\n",
    "\n",
    "class BandGapLabler(BaseLabeler):\n",
    "    property = 'band_gap'\n",
    "    regex_pattern = re.compile(r'\\b(band[- ]?gaps?|bandgaps?|energy[- ]?gap|energy gap)\\b', re.I)\n",
    "    # You may need to define functions for semantic_label and llm_label if needed\n",
    "\n",
    "labeler = Labeling()\n",
    "labeler.add_labeler(BandGapLabler())\n",
    "\n",
    "database = get_plain_articledb('nlo')\n",
    "loader = Filter(database)\n",
    "\n",
    "\n",
    "save_labeled_paras = save_labeled_paras_wrapper('nlo_labeled')\n",
    "\n",
    "chain = loader + labeler + save_labeled_paras\n",
    "\n",
    "run_chains_with_extarction_history_multi_threads(\n",
    "    chain,\n",
    "    'test_file',\n",
    "    10,\n",
    "    'nlo_labeled'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144eedd8",
   "metadata": {},
   "source": [
    "## Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf23eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pastalover/miniconda3/envs/sisyphus_context/lib/python3.10/site-packages/pydantic/main.py:1552: RuntimeWarning: fields may not start with an underscore, ignoring \"__tablename__\"\n",
      "  warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"', RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "\n",
    "from sisyphus.chain.paragraph import Paragraph\n",
    "from sisyphus.chain.extract import BaseExtractor, Extraction\n",
    "\n",
    "# your models\n",
    "class Bandgap(BaseModel):\n",
    "    \"\"\"Extract bandgap information from scientific papers.\"\"\"\n",
    "    bandgap: Optional[str] = Field(description=\"Bandgap value with unit, e.g., '1.5 eV'\")\n",
    "    bandgap_type: Optional[Literal['direct', 'indirect']] = Field(description=\"Type of bandgap: direct or indirect\")\n",
    "    measurement_method: Optional[str] = Field(description=\"Method used to measure the bandgap, e.g., 'UV-Vis spectroscopy'\")\n",
    "\n",
    "class Records(BaseModel):\n",
    "    records: List[Bandgap]\n",
    "\n",
    "# you prompt, must contains text field\n",
    "nlo_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', 'you are a helpful assistant that extracts specific information from scientific papers.'),\n",
    "        ('user', '[START OF PAPER]\\n{text}\\n[END OF PAPER]\\n\\nInstruction:\\n{instruction}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "class BGExtractor(BaseExtractor):\n",
    "    target_properties = ['band_gap']\n",
    "    model = ChatOpenAI(model_name='gpt-4.1', temperature=0)\n",
    "\n",
    "    def create_model_prompt(self, paragraphs): # note the paragraphs are merged paragraphs\n",
    "        for paragraph in paragraphs:\n",
    "            paragraph.set_pydantic_model(Records)\n",
    "            paragraph.set_prompt(\n",
    "                nlo_prompt,\n",
    "                {'instruction': 'Extract bandgap information including bandgap value with unit, bandgap type (direct or indirect), and measurement method from the given scientific paper paragraph. Provide the results in a structured format.'\n",
    "                }\n",
    "            )\n",
    "\n",
    "def load_from_labeled_db(docs):\n",
    "    return [Paragraph.from_labeled_document(doc, id_) for id_, doc in enumerate(docs)]\n",
    "\n",
    "extractor = Extraction()\n",
    "bg_extractor = BGExtractor()\n",
    "extractor.add_extractor(bg_extractor)\n",
    "\n",
    "from sisyphus.chain import Writer, Filter\n",
    "from sisyphus.utils.helper_functions import get_create_resultdb, get_plain_articledb\n",
    "\n",
    "\n",
    "db = get_plain_articledb('nlo_labeled')\n",
    "loader = Filter(db)\n",
    "result_db = get_create_resultdb('nlo_results')\n",
    "writer = Writer(result_db)\n",
    "\n",
    "chain = loader + load_from_labeled_db + extractor + writer\n",
    "chain.compose('10.1002&sol;adfm.201801589.html') # example file name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b828135",
   "metadata": {},
   "source": [
    "#### Case view 1: multiple properties extracted parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dcc644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just add multiple labelers to Labeling instance and add multiple extractors to Extractor instance. It was built to support parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0e69d",
   "metadata": {},
   "source": [
    "#### Case view 2: multiple properties merged extraction\n",
    "- We use regex serach, semantic search and llm model to find target paragraphs\n",
    "- The targets are then merged to a broad context chunk and sent to llm. The result model is adjusted with the exists of properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb15084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions for labeling\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import dspy\n",
    "\n",
    "\n",
    "lm = dspy.LM('openai/gpt-4.1', max_tokens=3000)\n",
    "dspy.configure(lm=lm)\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "chroma_db = Chroma(collection_name='synthesis_embedding', embedding_function=embeddings)\n",
    "has_embedded = []\n",
    "\n",
    "def retrieve(vector_store, source, query, sub_titles, k=3):\n",
    "    if not sub_titles:\n",
    "        filter_ = {\"source\": source}\n",
    "    else:\n",
    "        filter_ = {\"$and\":[{\"sub_titles\": {\"$in\": sub_titles}}, {\"source\": source}]}\n",
    "    return vector_store.similarity_search(query, k=k, filter=filter_)\n",
    "\n",
    "def match_subtitles(docs, pattern):\n",
    "    sub_titles = list(set([doc.metadata[\"sub_titles\"] for doc in docs]))\n",
    "    target_titles = []\n",
    "    for title in sub_titles:\n",
    "        if pattern.search(title):\n",
    "            target_titles.append(title)\n",
    "    return target_titles\n",
    "\n",
    "def find_candidates(paragraphs, doc_candidates):\n",
    "    cands = []\n",
    "    for doc_candidate in doc_candidates:\n",
    "        for para in paragraphs:\n",
    "            if para.document == doc_candidate:\n",
    "                cands.append(para)\n",
    "                break\n",
    "    return cands\n",
    "\n",
    "def semantic_label_property(paragraphs, query, k):\n",
    "    \"\"\"label property in results section\"\"\"\n",
    "    source = paragraphs[0].metadata['source']\n",
    "    res_pattern = re.compile(r'result', re.I)\n",
    "    res_titles = match_subtitles(paragraphs, res_pattern)\n",
    "    if source not in has_embedded:\n",
    "        has_embedded.append(source)\n",
    "        docs = [paragraph.document for paragraph in paragraphs]\n",
    "        chroma_db.add_documents(docs)\n",
    "    similar_docs = retrieve(chroma_db, source, query, res_titles, k)\n",
    "    return find_candidates(paragraphs, similar_docs)\n",
    "\n",
    "def semantic_label_synthesis(paragraphs, query, k):\n",
    "    \"\"\"label synthesis in method section\"\"\"\n",
    "    source = paragraphs[0].metadata['source']\n",
    "    syn_pattern = re.compile(r'(experiment)|(preparation)|(method)', re.I)\n",
    "    method_titles = match_subtitles(paragraphs, syn_pattern)\n",
    "    if source not in has_embedded:\n",
    "        has_embedded.append(source)\n",
    "        docs = [paragraph.document for paragraph in paragraphs]\n",
    "        chroma_db.add_documents(docs)\n",
    "    similar_docs = retrieve(chroma_db, source, query, method_titles, k)\n",
    "    return find_candidates(paragraphs, similar_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d25a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Literal\n",
    "\n",
    "import dspy\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from sisyphus.heas.tabel import LabelTablesStrength\n",
    "from sisyphus.utils.helper_functions import get_plain_articledb, get_create_resultdb\n",
    "from sisyphus.chain.label import BaseLabeler, Labeling, save_labeled_paras_wrapper\n",
    "from sisyphus.chain.chain_elements import Filter, Writer, run_chains_with_extarction_history_multi_threads\n",
    "\n",
    "\n",
    "class StrengthLabeler(BaseLabeler):\n",
    "    property = 'strength'\n",
    "    regex_pattern = re.compile(r'(\\b(MPa|GPa)\\b|\\d+(\\.\\d+)?\\s*%)')\n",
    "    query = \"The stress-strain curve of alloy, describes yield strength (ys), tensile strength (uts) and elongation properties, for example, CoCuFeMnNi shows tensile strength of 1300 MPa and total elongation of 20%.\"\n",
    "\n",
    "    def semantic_label(self, paragraphs):\n",
    "        return semantic_label_property(paragraphs, self.query, k=5)\n",
    "\n",
    "class LabelTablesStrength(dspy.Signature):\n",
    "    \"\"\"You are an expert in materials science and mechanical testing. Given the following CSV table from a scientific paper on high entropy alloys, determine whether it contains at least one tensile or compressive test property.\n",
    "\n",
    "    Relevant Properties for Classification:\n",
    "    A table should be classified as relevant if it contains at least one of the following:\n",
    "\n",
    "    Yield Strength (YS) (MPa)\n",
    "    Ultimate Tensile Strength (UTS) (MPa)\n",
    "    Compressive Strength (MPa)\n",
    "    Strain (percentage or as a ratio, e.g., true strain or elongation)\n",
    "    Exclusions:\n",
    "    Do not classify table as relevant if they only mention:\n",
    "\n",
    "    Fracture strength\n",
    "    Hardness (e.g., Vickers, Brinell, Rockwell)\n",
    "    Fatigue strength\n",
    "    Shear strength\"\"\"\n",
    "    table: str = dspy.InputField()\n",
    "    contains: bool = dspy.OutputField()\n",
    "\n",
    "\n",
    "class StrengthTableLabler(BaseLabeler):\n",
    "    property = 'strength'\n",
    "    llm_labeler = dspy.ChainOfThought(LabelTablesStrength)\n",
    "\n",
    "    def llm_label(self, paragraph):\n",
    "        if not paragraph.is_table():\n",
    "            return False\n",
    "        contains = self.llm_labeler(table=paragraph.page_content).contains\n",
    "        return contains\n",
    "\n",
    "class PhaseLabeler(BaseLabeler):\n",
    "    property = 'phase'\n",
    "    regex_pattern = re.compile(r'\\b(FCC|BCC|HCP|L12|B2|Laves|f.c.c.|b.c.c.|h.c.p.|face-centered cubic|body-centered cubic|hexagonal close-packed|intermetallic|IM)\\b', re.I)\n",
    "    query = \"Microstructure characterization of alloys (common phases include FCC, BCC, HCP, L12, B2 etc.), usually through technique like XRD or TEM. Describe about phase and grain size and boundaries\"\n",
    "\n",
    "    def semantic_label(self, paragraphs):\n",
    "        return semantic_label_property(paragraphs, self.query, k=5)\n",
    "\n",
    "class ClassifySyn(dspy.Signature):\n",
    "    \"\"\"assign topic to paragraphs of HEAs(high entropy alloys) papers. The topics include synthesis, characterization, and others.\n",
    "    Note: a qualified synthesis paragraph should include the synthesis and processing of materials, including methods such as melting, casting, rolling, annealing, mechnical processes or additive manufacturing. be very strict about your decision.\"\"\"\n",
    "    paragraph: str = dspy.InputField()\n",
    "    topic: Literal['synthesis', 'characterization', 'others'] = dspy.OutputField()\n",
    "\n",
    "class ExperimentalLabeler(BaseLabeler):\n",
    "    property = 'synthesis'\n",
    "    query = \"Experimental procedures describing the synthesis and processing of HEAs materials, including methods such as melting, casting, rolling, annealing, heat treatment, or other fabrication techniques. Details often include specific temperatures (e.g., °C), durations (e.g., hours, minutes), atmospheric conditions (e.g., argon, vacuum), mechanical deformation (e.g., rolling reduction).\"\n",
    "    llm_labeler = dspy.ChainOfThought(ClassifySyn)\n",
    "\n",
    "    def semantic_label(self, paragraphs):\n",
    "        return semantic_label_synthesis(paragraphs, self.query, k=3)\n",
    "    \n",
    "    def llm_label(self, paragraph):\n",
    "        topic = self.llm_labeler(paragraph=paragraph.page_content).topic\n",
    "        return topic == 'synthesis'\n",
    "\n",
    "labeler = Labeling()\n",
    "labeler.add_labeler(StrengthLabeler())\n",
    "labeler.add_labeler(StrengthTableLabler())\n",
    "labeler.add_labeler(PhaseLabeler())\n",
    "labeler.add_labeler(ExperimentalLabeler())\n",
    "\n",
    "database = get_plain_articledb('heas_1531')\n",
    "loader = Filter(database)\n",
    "\n",
    "\n",
    "save_labeled_paras = save_labeled_paras_wrapper('heas_labeled')\n",
    "\n",
    "chain = loader + labeler + save_labeled_paras\n",
    "\n",
    "chain.compose('10.1002&sol;adem.201600726.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47cae4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models, instructions\n",
    "from ast import literal_eval\n",
    "import re\n",
    "\n",
    "from pydantic import BaseModel, Field, ConfigDict, field_validator\n",
    "from typing import List, Optional, Literal\n",
    "import json\n",
    "\n",
    "\n",
    "class Phase(BaseModel):\n",
    "    phases: List[str] = Field(description='list of phases present in the material')\n",
    "    test_env: Optional[str] = Field(description='other test parameters if avilable (not test method), be succinct, e.g., under plastic deformation')\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "class Strength(BaseModel):\n",
    "    ys: Optional[str] = Field(description=\"Yield strength with unit\")\n",
    "    uts: Optional[str] = Field(description=\"Ultimate tensile/compressive strength with unit\")\n",
    "    strain: Optional[str] = Field(description=\"Fracture strain. If in percentage form, please add '%' sign, else return as decimal form. Example: 0.5 or 50%\")\n",
    "    temperature: Optional[str] = Field(description=\"Test temperature with unit, if not specified, return 'room temperature'\")\n",
    "    strain_rate: Optional[str] = Field(description=\"Strain rate with unit, e.g., 1e-3 s^-1\")\n",
    "    test_env: Optional[str] = Field(description=\"Other tensile/compressive test environments if available, be succinct. e.g., sample geometry, salt environment\")\n",
    "    test_type: Literal['tensile', 'compressive']\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "class MetaData(BaseModel):\n",
    "    composition: str = Field(description='nominal material composition with basis marker, e.g., \"AlCoCrFeNi2.5@at\", \"AlCoCrFeNi2.1@wt\", \"AlCoCrFeNi@at+Al2O3@wt[5%]\"')\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "class Synthesis(BaseModel):\n",
    "    \"\"\"Synthesis information for a HEAs material.\n",
    "    Note:\n",
    "    - Use article synthesis section as only source of information.\n",
    "    - Do not contain process related to test (e.g., tensile/compression test temperature) or characterization (e.g., XRD, SEM).\n",
    "    \"\"\"\n",
    "    steps: str = Field(\n",
    "        description=(\n",
    "            \"List of processing steps in chronological order as JSON objects. \"\n",
    "            \"Example: [\"\n",
    "            '{\"induction melting\": {\"power\": \"50 kW\", \"coil frequency\": \"10 kHz\", \"atmosphere\": \"argon\", '\n",
    "            '\"pressure\": \"1 atm\", \"crucible material\": \"graphite\", \"liquid mixing time\": \"5 min\", \"number of remelts\": \"2\"}}, '\n",
    "            '{\"annealing\": {\"temperature\": \"800 K\", \"duration\": \"1 h\", \"atmosphere\": \"argon\"}}'\n",
    "            \"]. Return [] if no processing info is found; use empty strings for unknown values.\"\n",
    "        )\n",
    "    )\n",
    "    # Forbid additional properties so JSON Schema has additionalProperties=false\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "    # @field_validator('steps', mode='after')\n",
    "    # @classmethod\n",
    "    # def load(cls, value: str):\n",
    "    #     try:\n",
    "    #         value = json.loads(value)\n",
    "    #     except:\n",
    "    #         value = literal_eval(value)\n",
    "    #     return value\n",
    "\n",
    "    @field_validator('steps', mode='after')\n",
    "    @classmethod\n",
    "    def load(cls, value):\n",
    "\n",
    "        value = value.replace('\\x00', '').replace('\\u0000', '')\n",
    "\n",
    "        value = re.sub(r'[\\x00-\\x1F\\x7F]', '', value)\n",
    "\n",
    "        try:\n",
    "            return json.loads(value)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            return literal_eval(value)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd2e7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import create_model\n",
    "\n",
    "\n",
    "def create_result_model_dynamic(properties: List[Literal['strength', 'phase', 'grain_size']], has_synthesis: bool):\n",
    "    \"\"\"Dynamically create a Pydantic model for Records based on the requested properties using create_model.\"\"\"\n",
    "    fields = {\n",
    "        'metadata': (MetaData, ...)\n",
    "    }\n",
    "    if 'strength' in properties:\n",
    "        fields['strength'] = (List[Strength], Field(..., description='Create multiple strength items in list if multiple testing conditions (temperature, strain rate, environment) are reported and every condition has correspond strength value. Each entry should correspond to a unique set of testing conditions. Only using range if no other information is available.'))\n",
    "    if 'phase' in properties:\n",
    "        fields['phase'] = (List[Phase], Field(..., description='phase information'))\n",
    "    if has_synthesis:\n",
    "        fields['synthesis'] = (Synthesis, Field(..., description='synthesis information'))\n",
    "\n",
    "    Record = create_model('Record', __base__=BaseModel, **fields)\n",
    "    # ensure dynamic models forbid extra properties so the LM JSON schema includes additionalProperties=false\n",
    "    Record.model_config = ConfigDict(extra=\"forbid\")\n",
    "    doc = \"List of extracted records, encourage to split into multiple records if processing parameters varied\"\n",
    "    Records = create_model('Records', __base__=BaseModel, records=(List[Record], Field(..., description='list of extracted records')), __doc__=doc)\n",
    "    Records.model_config = ConfigDict(extra=\"forbid\")\n",
    "    return Records\n",
    "\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate(\n",
    "    [\n",
    "        ('system', 'you are a helpful assistant that extracts specific information from scientific papers.'),\n",
    "        ('user', '[START OF PAPER]\\n{text}\\n[END OF PAPER]\\n\\nInstruction:\\n{instruction}')\n",
    "    ]\n",
    ")\n",
    "phase_instruction = \"\"\"Extract phase information from the text\n",
    "hase types:\n",
    "FCC, BCC, HCP, B2, intermetallic compounds (e.g., TiNi, Ti₂Ni, γ' precipitates, silicides, aluminides, sigma (σ) phases), carbides (e.g., WC), oxides (e.g., SiO₂), amorphous phases.\n",
    "The dendrites themselves are made OF a phase (could be FCC, BCC, etc.), but \"dendritic\" itself is not a phase.\n",
    "Other Similar Terms to Exclude from Phase:\n",
    "Dendritic, equiaxed, columnar (grain morphology)\n",
    "Lamellar, eutectic (phase arrangement)\n",
    "Fine-grained, coarse-grained (grain size)\n",
    "\n",
    "Guideline for phase extraction:\n",
    "- If the author mentions ordered/disordered, include it in the phase information.\n",
    "- Same phase can be present multiple times, e.g., \"FCC, FCC\" extract as \"FCC, FCC\".\n",
    "- Main phase should be listed first, followed by secondary phases, and so on.\"\"\"\n",
    "\n",
    "strength_instruction = \"\"\"Extract mechanical property relevant to ys, uts and strain from the text\n",
    "Follow these rules:\n",
    "- If tested under different setting (temperature, strain rate, etc), extract separately\n",
    "- Only collect tensile/compressive strength and strain data, exclude all other mechanical properties.\n",
    "- Strain should refer to fracture strain only.\n",
    "- Prioritize table values over text if there is a conflict. \n",
    "- If the value provided is a range, for example, \"from 200 MPa to 300 MPa\", extract it as \"200-300 MPa\".\n",
    "- If the value is given as \"greater than\" or \"less than\", for example, \"greater than 400 MPa\", extract it as \">400 MPa\".\n",
    "- If the value is given as \"approximately\" or \"around\", for example, \"approximately 250 MPa\", extract it as \"~250 MPa\".\n",
    "- Otherwise, extract the value as it is.\"\"\"\n",
    "\n",
    "def create_instruction(properties):\n",
    "    instructions = []\n",
    "    if 'phase' in properties:\n",
    "        instructions.append(phase_instruction)\n",
    "    if 'strength' in properties:\n",
    "        instructions.append(strength_instruction)\n",
    "    return '\\n\\n'.join(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d19e2722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pastalover/miniconda3/envs/sisyphus_context/lib/python3.10/site-packages/pydantic/main.py:1552: RuntimeWarning: fields may not start with an underscore, ignoring \"__tablename__\"\n",
      "  warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"', RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from sisyphus.chain.extract import BaseExtractor, Extraction\n",
    "from sisyphus.chain.paragraph import Paragraph\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pydantic') # avoide the case that we convert json string to python object will trigger pydantic warning\n",
    "\n",
    "class Extractor(BaseExtractor):\n",
    "    target_properties = ['strength', 'phase', 'synthesis']\n",
    "    model = ChatOpenAI(model_name='gpt-4.1', temperature=0)\n",
    "    def create_model_prompt(self, paragraphs):\n",
    "        for paragraph in paragraphs:\n",
    "            model = create_result_model_dynamic(paragraph.property_types, paragraph.has_property('synthesis'))\n",
    "            paragraph.set_pydantic_model(model)\n",
    "            instruction = create_instruction(paragraph.property_types)\n",
    "            paragraph.set_prompt(\n",
    "                prompt_template,\n",
    "                {'instruction': instruction}\n",
    "            )\n",
    "\n",
    "\n",
    "def load_from_labeled_db(docs):\n",
    "    return [Paragraph.from_labeled_document(doc, id_) for id_, doc in enumerate(docs)]\n",
    "\n",
    "extractor = Extraction()\n",
    "hea_extractor = Extractor()\n",
    "extractor.add_extractor(hea_extractor)\n",
    "\n",
    "from sisyphus.chain import Writer, Filter\n",
    "from sisyphus.utils.helper_functions import get_create_resultdb, get_plain_articledb\n",
    "\n",
    "\n",
    "db = get_plain_articledb('heas_labeled')\n",
    "loader = Filter(db)\n",
    "result_db = get_create_resultdb('heas_results')\n",
    "writer = Writer(result_db)\n",
    "\n",
    "chain = loader + load_from_labeled_db + extractor + writer\n",
    "chain.compose('10.1002&sol;adem.201600726.html') # example file name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c8eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sisyphus_context",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
