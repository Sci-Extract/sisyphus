{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71cbb0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mInitializing Langtrace SDK..\u001b[39m\n",
      "\u001b[37mâ­ Leave our github a star to stay on top of our updates - https://github.com/Scale3-Labs/langtrace\u001b[39m\n",
      "Skipping openai due to error while instrumenting: No module named 'openai.resources.responses'\n",
      "\u001b[34mExporting spans to Extraction..\u001b[39m\n",
      "\u001b[34mLangtrace Project URL: https://app.langtrace.ai/project/cmf58kthl000f5i55fmxndtpw/traces\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pastalover/miniconda3/envs/sisyphus_context/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langtrace_python_sdk import langtrace\n",
    "langtrace.init(api_key = '8e0dafdc118df1613b10dbdda776b0b062427b0a9b1cb18b719688714e1ea445',\n",
    "  disable_tracing_for_functions= {\n",
    "    \"open_ai\": [ # All supported functions for openai\n",
    "      'openai.embeddings.create',\n",
    "    ]\n",
    "  },\n",
    "  disable_instrumentations={\"all_except\": ['openai']}\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n",
    "from typing import Callable\n",
    "from functools import partial\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from sisyphus.chain import Filter, Writer\n",
    "from sisyphus.utils.helper_functions import get_plain_articledb, get_create_resultdb\n",
    "from sisyphus.urgent.json_schemas import StrengthRecords, PhaseRecords, GrainSizeRecords\n",
    "import sisyphus.urgent.json_schemas_no_syn\n",
    "from sisyphus.chain import Paragraph, ParagraphExtend\n",
    "from sisyphus.strategy.utils import get_paras_with_props, get_synthesis_paras\n",
    "from sisyphus.urgent.properties_extraction import extract_func_wrapper\n",
    "from sisyphus.urgent.entity_resolution import entity_resolution_llms, entity_resolution_rule\n",
    "from sisyphus.urgent.merge import merge, REFERRED\n",
    "\n",
    "from prompt import simple_prompt_template, simple_prompt_template_no_syn, phase_instruction, strength_instruction\n",
    "\n",
    "    \n",
    "model = ChatOpenAI(temperature=0, model='gpt-4.1')\n",
    "\n",
    "def _make_extractor(prompt_template, output_model, labels, instruction, type_name, has_synthesis=True):\n",
    "    \"\"\"Helper to create an extractor partial to avoid repeated boilerplate.\"\"\"\n",
    "    # Wrap the newer `extract_property_` which composes prompt + model\n",
    "    return partial(\n",
    "        extract_property_,\n",
    "        labels=labels,\n",
    "        has_synthesis=has_synthesis,\n",
    "        prompt_template=prompt_template,\n",
    "        instruction=instruction,\n",
    "        chat_model=model,\n",
    "        output_model=output_model,\n",
    "        type=type_name,\n",
    "    )\n",
    "\n",
    "def extract_property_(\n",
    "        paragraphs: list[Paragraph],\n",
    "        labels: list[str],\n",
    "        has_synthesis: bool,\n",
    "        prompt_template: ChatPromptTemplate,\n",
    "        instruction: str,\n",
    "        chat_model: ChatOpenAI,\n",
    "        output_model: BaseModel,\n",
    "        **kwargs \n",
    ") -> ParagraphExtend:\n",
    "    chain = prompt_template | chat_model.with_structured_output(output_model, method='json_schema')\n",
    "\n",
    "    if has_synthesis:\n",
    "        target_paras = get_paras_with_props(paragraphs, *labels)\n",
    "        if not target_paras:\n",
    "            return []\n",
    "        paragraph = ParagraphExtend.from_paragraphs(target_paras, **kwargs)\n",
    "        syn_para = ParagraphExtend.from_paragraphs(get_synthesis_paras(paragraphs)) \n",
    "        res = chain.invoke(\n",
    "            {\n",
    "                'property_instruction': instruction,\n",
    "                'synthesis_para': syn_para.page_content,\n",
    "                'property': paragraph.page_content\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        target_paras = get_paras_with_props(paragraphs, *labels)\n",
    "        if not target_paras:\n",
    "            return []\n",
    "        paragraph = ParagraphExtend.from_paragraphs(target_paras, **kwargs)\n",
    "        res = chain.invoke(\n",
    "            {\n",
    "                'property_instruction': instruction,\n",
    "                'property': paragraph.page_content\n",
    "            }\n",
    "        )\n",
    "\n",
    "    paragraph.set_data(res.records)\n",
    "    if paragraph.data:\n",
    "        return [paragraph]\n",
    "    return []\n",
    "\n",
    "# Extractors when synthesis paragraphs exist\n",
    "extract_phase = _make_extractor(\n",
    "    simple_prompt_template, PhaseRecords,\n",
    "    ['phase', 'composition'], phase_instruction, 'phase_extraction'\n",
    ")\n",
    "extract_strength = _make_extractor(\n",
    "    simple_prompt_template, StrengthRecords,\n",
    "    ['strength', 'composition', 'strain_rate'], strength_instruction, 'strength_extraction'\n",
    ")\n",
    "extract_grainsize = _make_extractor(\n",
    "    simple_prompt_template, GrainSizeRecords,\n",
    "    ['grain_size', 'composition'],\n",
    "    \"Extract the grain size information with units from the provided text. Do not record any other type of size information.\",\n",
    "    'grain_size_extraction'\n",
    ")\n",
    "\n",
    "# Extractors when synthesis paragraphs do NOT exist\n",
    "extract_phase_no_syn = _make_extractor(\n",
    "    simple_prompt_template_no_syn, sisyphus.urgent.json_schemas_no_syn.PhaseRecords,\n",
    "    ['phase', 'composition'], phase_instruction, 'phase_extraction_no_syn', has_synthesis=False\n",
    ")\n",
    "extract_strength_no_syn = _make_extractor(\n",
    "    simple_prompt_template_no_syn, sisyphus.urgent.json_schemas_no_syn.StrengthRecords,\n",
    "    ['strength', 'composition', 'strain_rate'], strength_instruction, 'strength_extraction_no_syn', has_synthesis=False\n",
    ")\n",
    "extract_grainsize_no_syn = _make_extractor(\n",
    "    simple_prompt_template_no_syn, sisyphus.urgent.json_schemas_no_syn.GrainSizeRecords,\n",
    "    ['grain_size', 'composition'],\n",
    "    \"Extract the grain size information with units from the provided text. Do not record any other type of size information.\",\n",
    "    'grain_size_extraction_no_syn', has_synthesis=False\n",
    ")\n",
    "\n",
    "\n",
    "def extract(paragraphs: list[Paragraph]):\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    merged = []\n",
    "    if syn_paras:=get_synthesis_paras(paragraphs):\n",
    "        extractors = (extract_phase, extract_strength, extract_grainsize)\n",
    "        expected_names = ['extract_phase', 'extract_strength', 'extract_grainsize']\n",
    "        name_to_result = {name: [] for name in expected_names}\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=3) as ex:\n",
    "            future_to_name = {ex.submit(fn, paragraphs): name for fn, name in zip(extractors, expected_names)}\n",
    "            for fut in as_completed(future_to_name):\n",
    "                name = future_to_name[fut]\n",
    "                try:\n",
    "                    name_to_result[name] = fut.result() or []\n",
    "                except Exception:\n",
    "                    # keep failure of one extractor from stopping others\n",
    "                    name_to_result[name] = []\n",
    "\n",
    "        paras = (name_to_result.get('extract_phase', []) +\n",
    "                 name_to_result.get('extract_strength', []) +\n",
    "                 name_to_result.get('extract_grainsize', []))\n",
    "\n",
    "        records_groups = []\n",
    "        records = []\n",
    "        for para in paras:\n",
    "            records_groups.append(para.data)\n",
    "            records_groups = [group for group in records_groups if group]  # filter out empty groups\n",
    "            records.extend(para.data)\n",
    "            records = [record for record in records if record]  # filter out empty records\n",
    "        if records:\n",
    "            metadata_referred = [record.metadata.model_dump() for record in records if getattr(record, REFERRED)]\n",
    "            metadata_all = [record.metadata.model_dump() for record in records]\n",
    "            metadata_groups = [[record.metadata.model_dump() for record in group if not getattr(record, REFERRED)] for group in records_groups]\n",
    "\n",
    "            if len(metadata_groups) > 1:\n",
    "                syn_text = ParagraphExtend.from_paragraphs(syn_paras).page_content\n",
    "                resolved_metadata_groups = entity_resolution_llms(metadata_groups, model, syn_text) + entity_resolution_rule(metadata_referred, ['composition', 'label'])\n",
    "            else:  # fallback to rule-based if only one group\n",
    "                resolved_metadata_groups = entity_resolution_rule(metadata_all, ['composition', 'label'])\n",
    "\n",
    "            merged = merge(resolved_metadata_groups, records)\n",
    "\n",
    "    else:\n",
    "        extractors = (extract_phase_no_syn, extract_strength_no_syn, extract_grainsize_no_syn)\n",
    "        expected_names = ['extract_phase_no_syn', 'extract_strength_no_syn', 'extract_grainsize_no_syn']\n",
    "        name_to_result = {name: [] for name in expected_names}\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=3) as ex:\n",
    "            future_to_name = {ex.submit(fn, paragraphs): name for fn, name in zip(extractors, expected_names)}\n",
    "            for fut in as_completed(future_to_name):\n",
    "                name = future_to_name[fut]\n",
    "                try:\n",
    "                    name_to_result[name] = fut.result() or []\n",
    "                except Exception:\n",
    "                    name_to_result[name] = []\n",
    "\n",
    "        paras = (name_to_result.get('extract_phase_no_syn', []) +\n",
    "                 name_to_result.get('extract_strength_no_syn', []) +\n",
    "                 name_to_result.get('extract_grainsize_no_syn', []))\n",
    "\n",
    "        records = []\n",
    "        for para in paras:\n",
    "            records.extend(para.data)\n",
    "            records = [record for record in records if record]  # filter out empty records\n",
    "        if records:\n",
    "            metadata_all = [record.metadata.model_dump() for record in records]\n",
    "            resolved_metadata_groups = entity_resolution_rule(metadata_all, ['composition', 'label'])\n",
    "\n",
    "            merged = merge(resolved_metadata_groups, records)\n",
    "\n",
    "    if merged:\n",
    "        with open('merged_records_debug.jsonl', 'a') as f:\n",
    "            import json\n",
    "            to_write = {\n",
    "                'DOI': paragraphs[0].metadata.get('doi'),\n",
    "                'records': merged\n",
    "            }\n",
    "            json_str = json.dumps(to_write, ensure_ascii=False)\n",
    "            f.write(json_str + \"\\n\")\n",
    "    return paras\n",
    "\n",
    "\n",
    "# from sisyphus.chain.chain_elements import run_chains_with_extarction_history_multi_threads\n",
    "# run_chains_with_extarction_history_multi_threads(extract_chain, 'heas_test', 5, 'urgent_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24cdfe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pastalover/miniconda3/envs/sisyphus_context/lib/python3.10/site-packages/pydantic/main.py:1552: RuntimeWarning: fields may not start with an underscore, ignoring \"__tablename__\"\n",
      "  warnings.warn(f'fields may not start with an underscore, ignoring \"{f_name}\"', RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "labeled_database = get_plain_articledb('heas_labeled_extend')\n",
    "labeled_getter = Filter(labeled_database)\n",
    "result_db = get_create_resultdb('urgent_test')\n",
    "writer = Writer(result_db)\n",
    "def load(docs):\n",
    "    return [Paragraph.from_labeled_document(doc, id_) for id_, doc in enumerate(docs)]\n",
    "extract_chain = labeled_getter + load + extract + writer\n",
    "extract_chain.compose('10.1002&sol;adem.201900587.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sisyphus_context",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
